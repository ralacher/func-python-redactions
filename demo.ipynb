{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "Nothing special here. Using `fitz` (`pymupdf`) to manipulate the PDF file and some Azure SDKs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.functions as func\n",
    "import requests\n",
    "import logging\n",
    "import json\n",
    "import fitz\n",
    "import io\n",
    "import os\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.storage.blob import BlobServiceClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Variables\n",
    "Read from `local.settings.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "with open('local.settings.json') as f:\n",
    "    settings = json.load(f)\n",
    "\n",
    "for key, value in settings['Values'].items():\n",
    "    os.environ[key] = str(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Blob Client and Download File\n",
    "Nothing special here. Input your own file and container names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = 'Case-5.pdf'\n",
    "CONTAINER_NAME = 'inbound'\n",
    "blob_client = BlobServiceClient.from_connection_string(os.environ[\"STORAGE_CONNECTION_STRING\"]).get_blob_client(container=CONTAINER_NAME, blob=FILENAME)\n",
    "data = blob_client.download_blob()\n",
    "myblob = data.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Form Recognizer Custom Extraction Models\n",
    "Object containing model IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = {\n",
    "    \"Headstone Application\": \"2024-07-26-v1\",\n",
    "    \"Invoice\": \"\",\n",
    "    \"Medical Record\": \"2024-07-30-v1\",\n",
    "    \"Quote\": \"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Form Recognizer Classification\n",
    "Function will take the blob file and return a classification response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_classification(blob):\n",
    "    endpoint = os.environ[\"FORM_RECOGNIZER_ENDPOINT\"]\n",
    "    key = os.environ[\"FORM_RECOGNIZER_KEY\"]\n",
    "    model_id = os.environ[\"FORM_RECOGNIZER_CLASSIFIER_MODEL_ID\"]\n",
    "\n",
    "    document_analysis_client = DocumentAnalysisClient(\n",
    "        endpoint=endpoint, credential=AzureKeyCredential(key)\n",
    "    )\n",
    "\n",
    "    # Make sure your document's type is included in the list of document types the custom model can analyze\n",
    "    #with open(blob.read(), 'rb') as f:\n",
    "    poller = document_analysis_client.begin_classify_document(model_id, blob)\n",
    "    result = poller.result()\n",
    "    return result.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Form Recognizer Extraction\n",
    "Function will take blob content and return OCR results, extracted key-value pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_content(blob, model_id, pages):\n",
    "    endpoint = os.environ[\"FORM_RECOGNIZER_ENDPOINT\"]\n",
    "    key = os.environ[\"FORM_RECOGNIZER_KEY\"]\n",
    "\n",
    "    document_analysis_client = DocumentAnalysisClient(\n",
    "        endpoint=endpoint, credential=AzureKeyCredential(key)\n",
    "    )\n",
    "\n",
    "    # Make sure your document's type is included in the list of document types the custom model can analyze\n",
    "    #with open(blob.read(), 'rb') as f:\n",
    "    poller = document_analysis_client.begin_analyze_document(model_id, blob, pages=pages)\n",
    "    result = poller.result()\n",
    "    return result.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Service Get PII Entities\n",
    "Function will call Language Service on blob content and return identified PII."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pii_entities(blob):\n",
    "    endpoint = os.environ[\"LANGUAGE_ENDPOINT\"]\n",
    "    key = os.environ[\"LANGUAGE_KEY\"]\n",
    "\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "        endpoint=endpoint, credential=AzureKeyCredential(key)\n",
    "    )\n",
    "\n",
    "    result = text_analytics_client.recognize_pii_entities(blob)\n",
    "    entities = [{'text': entity.text, 'category': entity.category} for entity in result[0].entities]\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Categorization\n",
    "Call OpenAI with the entities identified by Language Service PII."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_chatgpt_response(entities, content):\n",
    "    GPT4V_KEY = os.environ[\"OPENAI_KEY\"]\n",
    "    GPT4V_ENDPOINT = os.environ[\"OPENAI_ENDPOINT\"]\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"api-key\": GPT4V_KEY,\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": '''\n",
    "                    You are an AI assistant. Your objective is to identify personally identifiable information. Identify all PII in the provided documents besides the information belonging to the deceased/victim.\n",
    "                    Titles or job positions are not considered PII.\n",
    "                    Results should be a JSON array of PII values.\n",
    "                    JSON response should be plain JSON.\n",
    "                    {content}\n",
    "                    '''.format(content=content)\n",
    "                    },\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"response_format\": {\"type\": \"json_object\"},\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": '''\n",
    "                        Give me a JSON array containing valid PII entities for the following scenario. \n",
    "                        I am trying to redact PII from medical records. \n",
    "                        I have a list of PII from the medical record. \n",
    "                        The PII I want to redact belongs to all individuals who are not the victim or the deceased mentioned in the medical record.\n",
    "                        Titles or job positions are not considered PII and should not be included in the result.\n",
    "\n",
    "                        The below JSON array are the identified PII entities:\n",
    "                        {entities}\n",
    "\n",
    "                        The PII entities were derived from the below text:\n",
    "                        {content} \n",
    "                        '''\n",
    "                    },\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.95,\n",
    "        \"max_tokens\": 800\n",
    "    }\n",
    "\n",
    "    # Send request\n",
    "    try:\n",
    "        response = requests.post(GPT4V_ENDPOINT, headers=headers, json=payload)\n",
    "        response.raise_for_status()  # Will raise an HTTPError if the HTTP request returned an unsuccessful status code\n",
    "    except requests.RequestException as e:\n",
    "        raise SystemExit(f\"Failed to make the request. Error: {e}\")\n",
    "\n",
    "    # Handle the response as needed (e.g., print or process)\n",
    "    content = response.json()['choices'][0]['message']['content'].replace('```', '').replace('json', '')\n",
    "    logging.info(content)\n",
    "    return json.loads(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Redactions\n",
    "Given the blob file, Form Recognizer outputs, and OpenAI categorizations, draw bounding boxes on PDF document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def redact_pdf(blob, form, redactions):\n",
    "    # Open the PDF\n",
    "    try:\n",
    "        doc = fitz.open('pdf', blob)\n",
    "    except:\n",
    "        raise SystemExit(f'Failed to open the PDF file')\n",
    "\n",
    "    # Split redactions\n",
    "    redactions = set(word for name in redactions for word in name.split())\n",
    "\n",
    "    # Get the page\n",
    "    for page in doc.pages():\n",
    "        print(f'Starting redactions on page {page.number}')\n",
    "        page = doc.load_page(page.number)\n",
    "        scale_x = page.rect[2] / form['pages'][page.number]['width']\n",
    "        scale_y = page.rect[3] / form['pages'][page.number]['height']\n",
    "\n",
    "        # Create retractions from extracted values\n",
    "        for key, value in form['documents'][0]['fields'].items():\n",
    "            try:\n",
    "                polygon = value['bounding_regions'][0]['polygon']\n",
    "            except:\n",
    "                continue\n",
    "            print(f'Redacting: Key {key} with content {value[\"content\"]} on page {page.number}')\n",
    "            scaled_polygon = []\n",
    "            for i in range(0, len(polygon), 2):\n",
    "                x = polygon[i]['x'] * scale_x\n",
    "                y = polygon[i]['y'] * scale_y\n",
    "                scaled_polygon.extend([x, y])\n",
    "            min_x = min(scaled_polygon[0::2])\n",
    "            max_x = max(scaled_polygon[0::2])\n",
    "            min_y = min(scaled_polygon[1::2])\n",
    "            max_y = max(scaled_polygon[1::2])\n",
    "            rect = fitz.Rect(min_x, min_y, max_x, max_y)\n",
    "\n",
    "            page.add_redact_annot(rect)\n",
    "            page.apply_redactions()\n",
    "            page.apply_redactions(images=fitz.PDF_REDACT_IMAGE_NONE)\n",
    "            page.draw_rect(rect, color=(0,0,0), fill=(0,0,0))\n",
    "\n",
    "        # Create retractions from content\n",
    "        for word in form['pages'][page.number]['words']:\n",
    "            if word['content'] in redactions:\n",
    "                try:\n",
    "                    polygon = word['polygon']\n",
    "                except:\n",
    "                    continue\n",
    "                print(f'Redacting: {word[\"content\"]} on page {page.number}')\n",
    "\n",
    "                scaled_polygon = []\n",
    "                for i in range(0, len(polygon), 2):\n",
    "                    x = polygon[i]['x'] * scale_x\n",
    "                    y = polygon[i]['y'] * scale_y\n",
    "                    scaled_polygon.extend([x, y])\n",
    "                min_x = min(scaled_polygon[0::2])\n",
    "                max_x = max(scaled_polygon[0::2])\n",
    "                min_y = min(scaled_polygon[1::2])\n",
    "                max_y = max(scaled_polygon[1::2])\n",
    "                rect = fitz.Rect(min_x, min_y, max_x, max_y)\n",
    "\n",
    "                page.add_redact_annot(rect)\n",
    "                page.apply_redactions()\n",
    "                page.apply_redactions(images=fitz.PDF_REDACT_IMAGE_NONE)\n",
    "                page.draw_rect(rect, color=(0,0,0), fill=(0,0,0))\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blob contains 1 documents\n",
      "Redacting Medical Record document found in Case-5.pdf\n",
      "Processing pages 1-2\n",
      "Language Service found entities [{'text': 'Victim', 'category': 'PersonType'}, {'text': 'Jessica Brown', 'category': 'Person'}, {'text': '35', 'category': 'Quantity'}, {'text': '2024-07-20', 'category': 'DateTime'}, {'text': '2024-07-21', 'category': 'DateTime'}, {'text': 'Detective', 'category': 'PersonType'}, {'text': 'Detective', 'category': 'PersonType'}, {'text': 'Charles Green', 'category': 'Person'}, {'text': 'Officer', 'category': 'PersonType'}, {'text': 'Emily Brown', 'category': 'Person'}, {'text': 'Nurse', 'category': 'PersonType'}, {'text': 'Doctor', 'category': 'PersonType'}, {'text': 'Thomas Brown', 'category': 'Person'}, {'text': 'Medical Examiner', 'category': 'PersonType'}, {'text': 'Laura Wilson', 'category': 'Person'}, {'text': 'Jessica Brown', 'category': 'Person'}, {'text': '35-year-old', 'category': 'Quantity'}, {'text': 'Hospital Staff', 'category': 'PersonType'}, {'text': 'Thomas Brown', 'category': 'Person'}, {'text': 'Nurse', 'category': 'PersonType'}, {'text': 'Linda Davis', 'category': 'Person'}, {'text': 'Thomas Brown', 'category': 'Person'}, {'text': 'Thomas Brown', 'category': 'Person'}, {'text': 'Nurse', 'category': 'PersonType'}, {'text': 'Linda Davis', 'category': 'Person'}, {'text': 'patient', 'category': 'PersonType'}, {'text': 'Thomas Brown', 'category': 'Person'}, {'text': 'Nurse', 'category': 'PersonType'}, {'text': 'Linda Davis', 'category': 'Person'}, {'text': 'Nurse', 'category': 'PersonType'}, {'text': 'Linda Davis', 'category': 'Person'}, {'text': 'patient', 'category': 'PersonType'}, {'text': 'Jessica', 'category': 'Person'}, {'text': 'Jessica Brown', 'category': 'Person'}, {'text': 'newborn', 'category': 'PersonType'}]\n",
      "OpenAI found redactions ['Detective Charles Green', 'Officer Emily Brown', 'Dr. Thomas Brown', 'Dr. Laura Wilson', 'Nurse Linda Davis']\n",
      "Starting redactions on page 0\n",
      "Redacting: Brown on page 0\n",
      "Redacting: Detective on page 0\n",
      "Redacting: Charles on page 0\n",
      "Redacting: Green on page 0\n",
      "Redacting: Officer on page 0\n",
      "Redacting: Emily on page 0\n",
      "Redacting: Brown on page 0\n",
      "Redacting: Dr. on page 0\n",
      "Redacting: Thomas on page 0\n",
      "Redacting: Brown on page 0\n",
      "Redacting: Dr. on page 0\n",
      "Redacting: Laura on page 0\n",
      "Redacting: Wilson on page 0\n",
      "Redacting: Dr. on page 0\n",
      "Redacting: Thomas on page 0\n",
      "Redacting: Brown on page 0\n",
      "Redacting: Nurse on page 0\n",
      "Redacting: Linda on page 0\n",
      "Redacting: Davis on page 0\n",
      "Redacting: Dr. on page 0\n",
      "Redacting: Thomas on page 0\n",
      "Redacting: Brown on page 0\n",
      "Redacting: Dr. on page 0\n",
      "Redacting: Thomas on page 0\n",
      "Redacting: Brown on page 0\n",
      "Redacting: Nurse on page 0\n",
      "Redacting: Linda on page 0\n",
      "Redacting: Davis on page 0\n",
      "Redacting: Dr. on page 0\n",
      "Redacting: Thomas on page 0\n",
      "Redacting: Brown on page 0\n",
      "Starting redactions on page 1\n",
      "Redacting: Nurse on page 1\n",
      "Redacting: Linda on page 1\n",
      "Redacting: Davis on page 1\n",
      "Redacting: Nurse on page 1\n",
      "Redacting: Linda on page 1\n",
      "Redacting: Davis on page 1\n",
      "Redacting: Brown on page 1\n",
      "Wrote Case-5-redacted.pdf to container\n"
     ]
    }
   ],
   "source": [
    "documents = get_document_classification(myblob)\n",
    "print(f'Blob contains {len(documents[\"documents\"])} documents')\n",
    "for document in documents['documents']:\n",
    "    doc_type = document['doc_type']\n",
    "    model_id = MODELS[doc_type]\n",
    "    print(f'Redacting {doc_type} document found in {data.name}')\n",
    "\n",
    "    # Calculate range of page numbers per document\n",
    "    page_numbers = [region['page_number'] for region in document['bounding_regions']]\n",
    "    pages = page_numbers[0]\n",
    "    if len(page_numbers) > 0:\n",
    "        min_page = min(page_numbers)\n",
    "        max_page = max(page_numbers)\n",
    "        pages = f'{min_page}-{max_page}'\n",
    "\n",
    "    print(f'Found pages {pages}')\n",
    "    # Run custom extraction to get document content\n",
    "    form = get_document_content(myblob, model_id, pages)\n",
    "    \n",
    "    # Extract PII entities from document content\n",
    "    entities = get_pii_entities([form['content']])\n",
    "    print(f'Language Service found entities {entities}')\n",
    "\n",
    "    # Get redactions from OpenAI\n",
    "    redactions = get_chatgpt_response(entities, form['content'])\n",
    "    print(f'OpenAI found redactions {redactions}')\n",
    "\n",
    "    # Rename file\n",
    "    filename = f'{data.name.replace(\".pdf\", \"\").replace(\"inbound/\", \"\")}-redacted.pdf'\n",
    "    # Create new PDF file with redactions\n",
    "    output = redact_pdf(myblob, form, redactions)\n",
    "    # Save the redacted PDF to Azure Storage\n",
    "    stream = io.BytesIO()\n",
    "    output.save(stream)\n",
    "    output.close()\n",
    "    blob_client = BlobServiceClient.from_connection_string(os.environ[\"STORAGE_CONNECTION_STRING\"]).get_blob_client(container='outbound', blob=filename)\n",
    "    blob_client.upload_blob(stream.getvalue(), overwrite=True)\n",
    "    print(f'Wrote {filename} to container')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
